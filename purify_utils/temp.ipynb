{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhangh/workspace/FGWS-Detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "p = os.path.dirname(os.getcwd())  #获取要导入模块的上上级目录\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.path.append(\"..\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokenized_text =['[PAD]','happy']\n",
    "[1 if token!='[PAD]' else 0 for token in masked_tokenized_text ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_text  =\"Some weights of the model checkpoint at bert-base-uncased **3958y were not used when initializing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/zhangh/anaconda3/envs/allok/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text  =\"Some weights of the model checkpoint at bert-base-uncased were not used when initializing\"\n",
    "tokens = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=128,\n",
    "            add_special_tokens=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_str(string, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Parts adapted from https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/mydatasets.py\n",
    "    \"\"\"\n",
    "    assert isinstance(string, str)\n",
    "    string = string.replace(\"<br />\", \"\")\n",
    "    string = re.sub(r\"[^a-zA-Z0-9.]+\", \" \", string)\n",
    "\n",
    "    return (\n",
    "        string.strip().lower().split()\n",
    "        if tokenizer is None\n",
    "        else [t.text.lower() for t in tokenizer(string.strip())]\n",
    "    )\n",
    "\n",
    "def cut_raw(seq, max_len):\n",
    "    assert isinstance(seq, list)\n",
    "    return seq[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-mode MODE] [-model_type MODEL_TYPE]\n",
      "                             [-model_name MODEL_NAME] [-attack ATTACK]\n",
      "                             [-visible_devices VISIBLE_DEVICES] [-limit LIMIT]\n",
      "                             [-num_epoch NUM_EPOCH] [-dataset DATASET]\n",
      "                             [-fp_threshold FP_THRESHOLD]\n",
      "                             [-delta_thr DELTA_THR] [-gpu] [-attack_train_set]\n",
      "                             [-attack_val_set] [-detect_val_set]\n",
      "                             [-test_on_val] [-detect_baseline]\n",
      "                             [-tune_delta_on_val] [-voter_num VOTER_NUM]\n",
      "                             [-gaussian_step GAUSSIAN_STEP]\n",
      "                             [-use_gaussian USE_GAUSSIAN]\n",
      "                             [-purify_num PURIFY_NUM]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"4fd1eb96-3672-4d98-b595-8e7d1a05d774\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=/tmp/tmp-1818358gJS68IBq3g8x.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "from configLM import Config\n",
    "import csv\n",
    "config=Config()\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "\n",
    "def load_sst():\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(\"{}/{}.tsv\".format(config.path_to_sst2, \"train\")) as f:\n",
    "        print(\"self.config.path_to_sst2:\\n{self.config.path_to_sst2}\")\n",
    "        data = csv.reader(f, delimiter=\"\\t\")\n",
    "        header = False\n",
    "\n",
    "        for line in data:\n",
    "            if not header:\n",
    "                header = True\n",
    "            else:\n",
    "                \n",
    "                [text, label] = line\n",
    "                texts.append(text)\n",
    "                # cleaned = cut_raw(\n",
    "                #     clean_str(text, tokenizer=spacy_tokenizer),\n",
    "                #     config.bert_max_len,\n",
    "                # )\n",
    "                # texts.append(cleaned)\n",
    "                labels.append(int(label.replace(\"\\n\", \"\").strip()))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from configLM import Config\n",
    "import csv\n",
    "# config=Config()\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "path_to_sst2 = \"/home/zhangh/workspace/FGWS-Detection/data/data/sst2/tsv-format\"\n",
    "def load_sst():\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(\"{}/{}.tsv\".format(path_to_sst2, \"train\")) as f:\n",
    "        #print(\"self.config.path_to_sst2:\\n{self.config.path_to_sst2}\")\n",
    "        data = csv.reader(f, delimiter=\"\\t\")\n",
    "        header = False\n",
    "        \n",
    "        for line in data:\n",
    "            if not header:\n",
    "                header = True\n",
    "            else:\n",
    "                \n",
    "                [text, label] = line\n",
    "                texts.append(text)\n",
    "                # cleaned = cut_raw(\n",
    "                #     clean_str(text, tokenizer=spacy_tokenizer),\n",
    "                #     128,\n",
    "                # )\n",
    "                # texts.append(cleaned)\n",
    "                labels.append(int(label.replace(\"\\n\", \"\").strip()))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, _ = load_sst()\n",
    "print(len(text))\n",
    "_[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "best_epoch = (0, np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, inf)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhangh/workspace/FGWS-Detection/data\n"
     ]
    }
   ],
   "source": [
    "data_root_path =\"{}/data\" .format(os.path.dirname(os.getcwd()).format())\n",
    "print(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhangh/workspace/FGWS-Detection\n"
     ]
    }
   ],
   "source": [
    "data_root_path = os.path.dirname(os.getcwd())\n",
    "print(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2070, 15871, 1997, 1996, 2944, 26520, 2012, 14324, 1011, 2918, 1011, 4895, 28969, 2020, 2025, 2109, 2043, 3988, 6026]\n"
     ]
    }
   ],
   "source": [
    "token_ = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(token_)\n",
    "print(ids)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbbac4f32b1e7ed7599b18b0c4e6ec0488176a40b7411930deb2d52344aef062"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('allok': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
