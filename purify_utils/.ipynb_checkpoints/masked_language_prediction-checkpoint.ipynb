{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start pdf-converter OCR service\n",
    "\n",
    "Using this tool:\n",
    "https://github.com/D2P-APPS/pdf-ocr-tool\n",
    "\n",
    "Run the following command to run the container and start the webservice:\n",
    "\n",
    "    docker-compose up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up BERT masked language prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 08:28:02.606111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-24 08:28:02.606146: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Predicting neighbors to a word in sentence using BERTMaskedLM. \n",
    "# Neighbors are from BERT vocab (which includes subwords and full words) \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM,  AdamW\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import random\n",
    "import sys,os\n",
    "p = os.path.dirname(os.getcwd())  #获取要导入模块的上上级目录\n",
    "sys.path.append(p)\n",
    "from dataModule import process_imdb\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3,4,5'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "DEFAULT_MODEL_PATH='/home/zhangh/dataset/bert/bert-base-uncased'\n",
    "params_dir ='/home/zhangh/workspace/Attack-Word/data/model/bert_base_uncased/bert_base_model_beta.pkl'\n",
    "    \n",
    "DEFAULT_TO_LOWER=False\n",
    "DEFAULT_TOP_K = 10\n",
    "ACCRUE_THRESHOLD = 1\n",
    "\n",
    "def init_model(model_path,to_lower):\n",
    "    \"\"\"\n",
    "    Initiate BERTForMaskedLm model.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path,do_lower_case=to_lower)\n",
    "    model = BertForMaskedLM.from_pretrained(model_path)\n",
    "    #model.load_state_dict(torch.load(params_dir))\n",
    "    return model,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/zhangh/dataset/bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = init_model(DEFAULT_MODEL_PATH,to_lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from random import *\n",
    "import random\n",
    "def mask_words(words_list, max_pred=20):\n",
    "    '''\n",
    "    将文本中10%单词masked\n",
    "    words_list:存放一个文本中所有单词，开头是'[CLS]',结尾是'[SEP]'\n",
    "    '''\n",
    "    words = deepcopy(words_list)\n",
    "    #padding length=300,故为了防止mask到被截断的单词，需要处理超出长度的文本\n",
    "    if len(words)>300:\n",
    "        words = words[:301]\n",
    "    n_pred =  min(max_pred, max(1, int(len(words) * 0.15))) \n",
    "    # 15 % of tokens in one sentence\n",
    "    cand_maked_pos = [i for i, token in enumerate(words)\n",
    "                          if token != '[CLS]' and token != '[SEP]']\n",
    "    # candidate masked position\n",
    "    shuffle(cand_maked_pos)\n",
    "    masked_tokens, masked_pos = [], []\n",
    "    invalid_replaced = []\n",
    "    for token in ['[CLS]', '[SEP]','[PAD]']:    \n",
    "        invalid_replaced.append(tokenizer.convert_tokens_to_ids(token))\n",
    "    for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(words[pos])\n",
    "            if random.random() < 0.8:  # 80%\n",
    "                words[pos] = '[MASK]' # make mask\n",
    "            elif random.random() > 0.5:  # 10%\n",
    "                index = randint(0, tokenizer.vocab_size-1) \n",
    "                # random index in vocabulary\n",
    "                while index in invalid_replaced: # can't involve 'CLS', 'SEP', 'PAD'\n",
    "                   index = randint(0, tokenizer.vocab_size-1) \n",
    "                words[pos] = tokenizer.convert_ids_to_tokens(index) # replace\n",
    "    return words, masked_tokens, masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(traindata,tokenizer,max_len):\n",
    "    '''\n",
    "    imput\n",
    "    traindata:原始输入数据，存放str文本，list\n",
    "    tokenizer:模型的tokenizer\n",
    "    output\n",
    "    input_idxs:模型训练的输入indexs\n",
    "    target_idxs:模型训练的输出indexs\n",
    "    masked_tokens:masked的token\n",
    "    masked_pos:masked的token对应文本中的位置\n",
    "    '''\n",
    "    input_idxs = []\n",
    "    target_idxs = []\n",
    "    masked_tokens_all = []\n",
    "    masked_pos_all = []\n",
    "    for text in traindata:\n",
    "        org_tokenized_text = tokenizer.tokenize(text)\n",
    "        org_tokenized_text.insert(0, '[CLS]')\n",
    "        org_tokenized_text.append('[SEP]')\n",
    "\n",
    "        masked_tokenized_text,masked_tokens,masked_pos\\\n",
    "                            = mask_words(org_tokenized_text)\n",
    "        if len(org_tokenized_text)!= len(masked_tokenized_text):\n",
    "            print(\"error\")\n",
    "            continue\n",
    "        \n",
    "        #Padding\n",
    "        if len(org_tokenized_text)<max_len:\n",
    "            tmp=['[PAD]']*(max_len-len(org_tokenized_text))\n",
    "            org_tokenized_text.extend(tmp)\n",
    "            masked_tokenized_text.extend(tmp)\n",
    "        elif len(org_tokenized_text)>max_len:\n",
    "            org_tokenized_text = org_tokenized_text[:max_len]\n",
    "            masked_tokenized_text = masked_tokenized_text[:max_len]\n",
    "        \n",
    "        \n",
    "        target_indexed_tokens = tokenizer.convert_tokens_to_ids(org_tokenized_text)\n",
    "        input_indexed_tokens = tokenizer.convert_tokens_to_ids(masked_tokenized_text)\n",
    "        target_idxs.append(target_indexed_tokens)\n",
    "        input_idxs.append(input_indexed_tokens)\n",
    "        masked_tokens_all.append(masked_tokens)\n",
    "        masked_pos_all.append(masked_pos)\n",
    "    return input_idxs,target_idxs,masked_tokens_all,masked_pos_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 100, 3007, 103, 12410, 2003, 100, 1012, 100, 3007, 1997, 100, 2003, 100, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 100, 1996, 17974, 2005, 103, 103, 3791, 2000, 2022, 4225, 2306, 2023, 103, 1010, 103, 2323, 2655, 1996, 100, 6013, 5728, 2612, 1997, 2023, 2144, 1996, 2280, 3138, 2729]] [[101, 100, 3007, 1997, 100, 2003, 100, 1012, 100, 3007, 1997, 100, 2003, 100, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 100, 1996, 17974, 2005, 2830, 3413, 3791, 2000, 2022, 4225, 2306, 2023, 3853, 1010, 2028, 2323, 2655, 1996, 100, 6013, 5728, 2612, 1997, 2023, 2144, 1996, 2280, 3138, 2729]] [['of', '[UNK]'], ['pass', 'forward', 'one', 'while', 'function', 'processing']] [[3, 4], [6, 5, 15, 38, 13, 36]]\n"
     ]
    }
   ],
   "source": [
    "train_data_temp = [\"The capital of France is Paris. The capital of France is Paris.\",\\\n",
    "\"Although the recipe for forward pass needs to be defined within this function, \\\n",
    "one should call the Module instance afterwards instead of this since the former\\\n",
    " takes care of running the pre and post processing steps while the latter silently \\\n",
    " ignores them.\"]\n",
    "input_idxs,target_idxs,masked_tokens,masked_pos = create_dataset(train_data_temp,tokenizer,max_len=30)\n",
    "print(input_idxs,target_idxs,masked_tokens,masked_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "def loader(datas,tokenizer,batch_size=8):\n",
    "    input_idxs,target_idxs,masked_tokens,masked_pos = \\\n",
    "        create_dataset(datas,tokenizer,max_len=300)\n",
    "    train_set = TensorDataset(torch.LongTensor(input_idxs), torch.LongTensor(target_idxs))\n",
    "    train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True\n",
    "                          )\n",
    "    #print(\"batch_size: \",batch_size)\n",
    "    return train_loader,masked_tokens,masked_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1) sf\n",
      "(1, 3) aa\n",
      "(2, 4) bc\n",
      "(3, 4) are\n"
     ]
    }
   ],
   "source": [
    "x1=[1,3,4,4]\n",
    "x2=[\"sf\",\"aa\",\"bc\",\"are\"]\n",
    "for item1,item2 in zip(enumerate(x1),x2):\n",
    "    print(item1,item2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,tokenizer,dataset,epoch=4,batch_size= 4):\n",
    "    ''''''\n",
    "    model.train()  # 将模型设置为训练模式\n",
    "    model.to(device)\n",
    "    \n",
    "    train_loader,masked_tokens,masked_pos = loader(dataset,tokenizer,batch_size)\n",
    "    \n",
    "    avg_loss = []\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        for batch_idx,(input_idxs,target_idxs) in enumerate(train_loader):\n",
    "            input_idxs,target_idxs=input_idxs.to(device),target_idxs.to(device)\n",
    "            output = model(input_idxs,labels = target_idxs)\n",
    "            loss,logits = output[0],output[1]\n",
    "            loss = loss / batch_size  # 梯度积累\n",
    "            avg_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            if ((batch_idx + 1) % batch_size) == 0:\n",
    "                # 每 8 次更新一下网络中的参数\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            if batch_idx % 5 == 0:\n",
    "                logging.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss:{:.6f}'.format(\n",
    "                    e + 1, batch_idx, len(train_loader), 100. *\n",
    "                    batch_idx / len(train_loader), np.array(avg_loss).mean()\n",
    "                ))\n",
    "    print('Finished Training')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/zhangh/dataset/bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train files: 25000\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = init_model(DEFAULT_MODEL_PATH,to_lower=False)\n",
    "train_texts,train_labels = process_imdb.read_file('train',clearn_flag=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model,tokenizer,train_texts[:5],epoch=4,batch_size= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dir = '/home/zhangh/workspace/Attack-Word/data/model/mask_LM/bert_base_LM_IMDB.pkl'\n",
    "model.load_state_dict(torch.load(params_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test files: 25000\n"
     ]
    }
   ],
   "source": [
    "test_texts,trest_labels = process_imdb.read_file('test',clearn_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,tokenizer,test_set):\n",
    "    model.eval()\n",
    "    model= model.to(device)\n",
    "    print(\"Loading test dataset...\")\n",
    "    test_loader,masked_tokens,masked_pos = loader(test_set,tokenizer,batch_size=1)\n",
    "    print(\"Load test dataset!\")\n",
    "    #print(len(test_loader), len(masked_tokens),len(masked_posed))\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    acc_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for idx,(input,_) in enumerate(test_loader):\n",
    "            text_cor = 0\n",
    "            print(f\"test index = {idx}\")\n",
    "            input = input.to(device)\n",
    "            output = model(input)\n",
    "            pred_tokens = predict(tokenizer,output.logits[0],masked_pos[idx])#\n",
    "            # print(f\"mask num : {masked_pos[idx]}\")\n",
    "            # print(f\"result: {result_idx_tokens}\")\n",
    "            # print(f\"masked_token: {masked_tokens[idx]}\")\n",
    "            for i in range(len(masked_tokens)):\n",
    "                if masked_tokens[idx][i]==pred_tokens[i]: \n",
    "                    text_cor+= 1 \n",
    "            #text_acc = text_cor/len(input)\n",
    "            #print(f\"\")\n",
    "            correct+= text_cor\n",
    "            total += (len(masked_pos[idx]))\n",
    "        print(f\"正确预测的单词数量 {correct}，总数 {total},准确率 {100.*correct/total:.3f}%\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer,logits,masked_pos,topK_sample=0,max_sample=1):\n",
    "    \"\"\"\n",
    "    Guess masked tokens.\n",
    "    \"\"\"\n",
    "    #result_idx_tokens = {}\n",
    "    result = []\n",
    "    #print(f\"masked_pos: {masked_pos}\")\n",
    "    for pos in masked_pos:\n",
    "        #print(f\"pos:{pos}\")\n",
    "        # print(len(logits[pos])-1)\n",
    "        # print(len(logits[pos].tolist()))\n",
    "        preds = dict(zip(range(0,len(logits[pos])-1),logits[pos].tolist()))\n",
    "        #print(f\"preds: {preds}\")\n",
    "        #print(predictions.logits[0][idx])\n",
    "        #print(\"Average score: \",torch.mean(predictions.logits[0][idx],dim=0))\n",
    "        sorted_pred = OrderedDict(sorted(preds.items(), \n",
    "            key=lambda kv: kv[1], reverse=True))\n",
    "        idx = sample(sorted_pred,topK_sample,max_sample)\n",
    "        result.append(tokenizer.convert_ids_to_tokens(idx))\n",
    "        #result_idx_tokens[pos]=tokenizer.convert_ids_to_tokens(idx)\n",
    "    return result\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = sample(sorted_pred,topK_sample,max_sample)\n",
    "def softmax(v):\n",
    "    l1 = list(map(lambda x: np.exp(x), v))\n",
    "    return list(map(lambda x: x / sum(l1), l1))\n",
    "\n",
    "def sample(score_dict,topK_sample,max_sample):\n",
    "    ''''''\n",
    "    \n",
    "    if max_sample > 0 :#最大值采样\n",
    "        return list(score_dict.keys())[0]\n",
    "    else:#top_k 采样\n",
    "        pred_scores = list(score_dict.values())[:topK_sample]\n",
    "        pred_idxs = list(score_dict.keys())[:topK_sample]\n",
    "        probs = np.array(softmax(pred_scores))\n",
    "        pred_id = np.random.choice(a=pred_idxs, size=1, replace=True, p=probs)[0]\n",
    "        return pred_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "error\n",
      "Load test dataset!\n",
      "test index = 0\n",
      "test index = 1\n",
      "test index = 2\n",
      "test index = 3\n",
      "正确预测的单词数量 6，总数 70,准确率 8.571%\n"
     ]
    }
   ],
   "source": [
    "test_model(model,tokenizer,test_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "list1 = ['creepy', 'the', 'full', 'the', 'holm', 'script']\n",
    "list2 = ['creepy', 'this', 'full', 'time', 'holm', 'script']\n",
    "count=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creepy'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(17109)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(model,tokenizer,text,top_k=0,accrue_threshold=1):\n",
    "    \"\"\"\n",
    "    Guess masked tokens.\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    #print(tokenized_text)\n",
    "    print(len(tokenized_text))\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    #print(indexed_tokens)\n",
    "    print(len(indexed_tokens))\n",
    "    \n",
    "    # Create the segments tensors.\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "    masked_index = []\n",
    "    for i in range(len(tokenized_text)):\n",
    "        if (tokenized_text[i] == \"[MASK]\"):\n",
    "            masked_index.append(i) \n",
    "    # print(segments_ids)\n",
    "    # print(masked_index)\n",
    "    \n",
    "    results_dict = {}\n",
    "    accrue_threshold = 1\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "        # print(predictions.logits.shape) #1,81,30523\n",
    "        # print(predictions.logits[0].shape)\n",
    "        for idx in masked_index:\n",
    "            result_idx = {}\n",
    "            #print(predictions.logits[0][idx])\n",
    "            #print(\"Average score: \",torch.mean(predictions.logits[0][idx],dim=0))\n",
    "            count = 0\n",
    "            for i in range(len(predictions.logits[0][idx])):#30522\n",
    "                score_i = float(predictions.logits[0][idx][i].tolist())\n",
    "                if score_i > accrue_threshold :\n",
    "                #将位置i对应的token找到，并放入字典\n",
    "                    tok = tokenizer.convert_ids_to_tokens([i])[0]\n",
    "                    result_idx[tok] = float(score_i)\n",
    "                    count += 1\n",
    "            #print(f\"Valid prediction num {count} for idx: {idx}\")\n",
    "            results_dict[idx]=result_idx\n",
    "    sample(results_dict,top_k)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,tokenizer,top_k,accrue_threshold,text):\n",
    "    \"\"\"\n",
    "    Guess masked tokens.\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Create the segments tensors.\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "    masked_index = 0\n",
    "\n",
    "    for i in range(len(tokenized_text)):\n",
    "        if (tokenized_text[i] == \"[MASK]\"):\n",
    "            masked_index = i\n",
    "            break\n",
    "\n",
    "    #print(tokenized_text)\n",
    "    #print(masked_index)\n",
    "    results_dict = {}\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "        for i in range(len(predictions[0][0,masked_index])):\n",
    "            if (float(predictions[0][0,masked_index][i].tolist()) > accrue_threshold):\n",
    "                tok = tokenizer.convert_ids_to_tokens([i])[0]\n",
    "                results_dict[tok] = float(predictions[0][0,masked_index][i].tolist())\n",
    "\n",
    "    k = 0\n",
    "    sorted_d = OrderedDict(sorted(results_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    for i in sorted_d:\n",
    "        print(i,sorted_d[i])\n",
    "        k += 1\n",
    "        if (k > top_k):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    l1 = list(map(lambda x: np.exp(x), v))\n",
    "    return list(map(lambda x: x / sum(l1), l1))\n",
    "def max_sample():\n",
    "    ''''''\n",
    "def top_k_sample():\n",
    "    ''''''\n",
    "def sample(score_dict,top_k=0):\n",
    "    ''''''\n",
    "    \n",
    "    # if top_k > 0:\n",
    "    #     sampler = top_k_sample()\n",
    "    # else:\n",
    "    #     sampler = top_k_sample()\n",
    "    result = []\n",
    "    for idx in score_dict:\n",
    "        preds = score_dict[idx]\n",
    "        if top_k > 0 :\n",
    "            k=0\n",
    "            sorted_pred = OrderedDict(sorted(preds.items(), \n",
    "            key=lambda kv: kv[1], reverse=True))\n",
    "            for i in sorted_pred:\n",
    "                k += 1\n",
    "                if (k > top_k):\n",
    "                    sorted_pred[i] = 0\n",
    "            pred = sorted_pred           \n",
    "        scores = list(pred.values())\n",
    "        probs = np.array(softmax(scores))\n",
    "        pred_idxs = np.array(range(0,len(preds)))\n",
    "        tokens = list(pred.keys())\n",
    "        pred_id = np.random.choice(a=pred_idxs, size=1, replace=True, p=probs)[0]\n",
    "        #print(\"pred_id: \",pred_id)\n",
    "        result.append(tokens[pred_id])\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess redactions with BERT masked language prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentence from bay of pigs\n",
    "text = \"\"\"Although it cannot be determined accurately at\n",
    "what height any of the Brigade's B-26's actually were\n",
    "flying, Gar Thorsrud is of the opinion that they\n",
    "probably would have been cruising at 8,000'-10,000'\n",
    "for the early part of the trip, dropping down to\n",
    "2,000' when approximately 15 miles off the target\n",
    "by which time they would have been well past the\n",
    "Essex.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask \"height\"\n",
    "\n",
    "text = \"\"\"Although it cannot be determined accurately at\n",
    "what [MASK] any of the Brigade's B-26's actually were\n",
    "flying, Gar Thorsrud is of the opinion that they\n",
    "probably would have been cruising at 8,000'-10,000'\n",
    "for the early part of the trip, dropping down to\n",
    "2,000' when approximately 15 miles off the target\n",
    "by which time they would have been well past the\n",
    "Essex.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask \"target\"\n",
    "\n",
    "text = \"\"\"Although it cannot be determined accurately at\n",
    "what height any of the Brigade's B-26's actually were\n",
    "flying, Gar Thorsrud is of the opinion that they\n",
    "probably would have been cruising at 8,000'-10,000'\n",
    "for the early part of the trip, dropping down to\n",
    "2,000' when approximately 15 miles off the [MASK]\n",
    "by which time they would have been well past the\n",
    "Essex.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coast 8.16816234588623\n",
      "point 7.087375164031982\n",
      "ground 6.919796943664551\n",
      "road 6.64233922958374\n",
      "shore 6.514383792877197\n",
      "mark 6.252679824829102\n",
      "surface 6.111354827880859\n",
      "end 5.635773181915283\n",
      "target 5.60432767868042\n",
      "sea 5.528745174407959\n",
      "coastline 5.495854377746582\n"
     ]
    }
   ],
   "source": [
    "predict(model,tokenizer,DEFAULT_TOP_K,ACCRUE_THRESHOLD,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train files: 25000\n",
      "famous was \"famous\" for their tension and release style of cartoon where the semi-main character is in terrible peril, only to be rescued by the hero at the last second. this particular casper is the only one i can remember where death actually takes a hand. but even in death, there is still a happy ending.the constant in famous studios cartoons is that \"virtue always triumphs\". popeye always gets to his spinach in time, baby huey always out-foxes the fox, little audery always \"learns her lesson\". and some fs cartoons are really dark and depressing.you have to give them credit. as much as i love looney tunes and \"tom and jerry\" i don't think anyone was putting out a better cartoon product at that time than paramount. color, animation, music (the great winston sharples), editing, voices. they were consistent and a glowing example of the best that the art form had to offer.\n"
     ]
    }
   ],
   "source": [
    "train_texts,train_labels = process_imdb.read_file('train',clearn_flag=False)\n",
    "print(train_texts[0])\n",
    "# tokenizer.tokenize(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this enjoyable minor noir boasts a top cast, and many memorable scenes. the big distraction is the complete disregard for authentic accents. the spanish characters in the film are played by a frenchman (boyer), a belgian (francen), a greek (paxinou) and a hungarian (lorre)! and to top it all off bacall is supposed to be an english aristocrat! despite these absurdities, the performances are all very good - especially those of paxinou and lorre. but the scene in which boyer, paxinou and lorre meet, and talk in wildly different accents, is a real hoot! and i guess, seeing as how they were alone, that they should actually have been speaking in spanish anyway! it seems pretty weird that the brothers warner couldn't find any spanish speaking actors in los angeles! of course hollywood has often had an \"any old accent will do\" policy - my other favorite is greta garbo (swedish) as mata hari (dutch), who falls in love with a russian soldier played by a mexican (ramon novarro). maybe they should have got novarro for \"confidential agent\" - he would have been great in boyer's role or at least in francen's (which would have saved greatly on the dark make-up budget).\n"
     ]
    }
   ],
   "source": [
    "sentence_t = train_texts[66]\n",
    "print(sentence_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sentence(sentence, rate=0.1,mask_num=0):\n",
    "    words = sentence.split()\n",
    "    if mask_num < 1:\n",
    "        mask_num = int(len(words)*rate) if int(len(words)*rate)>0 else 1\n",
    "    mask_idxs = random.sample(range(0,len(words)-1), mask_num)\n",
    "    mask_info = {}\n",
    "    for idx in mask_idxs:\n",
    "        mask_info[idx] = words[idx]\n",
    "        words[idx] = '[MASK]'\n",
    "    return \" \".join(words), mask_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this enjoyable minor noir boasts a [MASK] cast, and many memorable scenes. [MASK] [MASK] distraction [MASK] the complete disregard for authentic accents. the spanish characters in the film are played by a frenchman (boyer), a belgian (francen), a greek (paxinou) and [MASK] hungarian (lorre)! and to top it all [MASK] bacall is [MASK] to be an english aristocrat! despite these absurdities, the performances are all very good [MASK] especially those of paxinou and lorre. but the scene in which boyer, paxinou and lorre meet, [MASK] talk in wildly different accents, is a real hoot! and i guess, seeing as how they were alone, [MASK] they should actually [MASK] been speaking in spanish anyway! it seems [MASK] weird that [MASK] brothers warner couldn't find [MASK] spanish speaking [MASK] in los angeles! of course hollywood [MASK] often had [MASK] \"any old accent will do\" policy - my other favorite is greta garbo (swedish) as mata hari [MASK] who falls in love with a russian soldier played by a mexican (ramon novarro). maybe they should have got novarro for \"confidential agent\" - he would have [MASK] great in boyer's role or at [MASK] in francen's (which would have saved greatly on the dark make-up budget). {136: 'an', 67: '-', 13: 'big', 182: 'been', 118: 'the', 115: 'pretty', 123: 'any', 107: 'have', 103: 'that', 49: 'off', 154: '(dutch),', 12: 'the', 126: 'actors', 6: 'top', 84: 'and', 189: 'least', 52: 'supposed', 41: 'a', 15: 'is', 133: 'has'}\n"
     ]
    }
   ],
   "source": [
    "mask_sent_t,mask_info  = mask_sentence(sentence_t,0.1)\n",
    "print(mask_sent_t,mask_info )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Although it cannot be determined accurately at\\nwhat height any of the Brigade's B-26's actually were\\nflying, Gar Thorsrud is of the opinion that they\\nprobably would have been cruising at 8,000'-10,000'\\nfor the early part of the trip, dropping down to\\n2,000' when approximately 15 miles off the [MASK]\\nby which time they would have been well past the\\nEssex.\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "268\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[6, 14, 15, 17, 55, 67, 72, 92, 118, 142, 146, 155, 158, 165, 168, 176, 179, 203, 238, 247]\n",
      "pred_id:  3\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  3\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  9\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "pred_id:  0\n",
      "['great', 'the', 'the', 'is', 'a', ',', 'supposed', ',', 'and', 'then', 'have', 'so', 'warner', 'a', 'house', 'had', 'an', ',', 'been', 'least']\n"
     ]
    }
   ],
   "source": [
    "predict_mask(model,tokenizer,mask_sent_t,top_k=10,accrue_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coast 8.16816234588623\n",
      "point 7.087375164031982\n",
      "ground 6.919796943664551\n",
      "road 6.64233922958374\n",
      "shore 6.514383792877197\n",
      "mark 6.252679824829102\n",
      "surface 6.111354827880859\n",
      "end 5.635773181915283\n",
      "target 5.60432767868042\n",
      "sea 5.528745174407959\n",
      "coastline 5.495854377746582\n"
     ]
    }
   ],
   "source": [
    "predict(model,tokenizer,DEFAULT_TOP_K,ACCRUE_THRESHOLD,text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
